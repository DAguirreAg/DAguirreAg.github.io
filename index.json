[{"content":"As the amount of personal webscraping projects increased, I realized the importance of having an structured project template. The reason being that it allows to systematically create new projects without having to worry about refactoring later on if it doesn\u0026rsquo;t match with my webscraping service.\nBecause of this, I have decided to explain in this post how I structure most of my webscraping projects:\n1. Considerations I like to keep each project as independent from the rest as possible. This allows me to continuously improve each project without worrying about breaking the depending systems. This is why I am a huge fan of employing venvs and containerizing my applications. On top of this I also prefer to keep the scheduling scripts (if any) close to the original repo, as I have seen how keeping everything separated makes the refactoring of existing projects way slower (see 2.2. Scheduling folder for more information).\n2. Main structure Most of my applications follow the following pattern:\n‚îú‚îÄ‚îÄ .git ‚îú‚îÄ‚îÄ .gitignore ‚îú‚îÄ‚îÄ LICENSE ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ source ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ dockerfile ‚îú‚îÄ‚îÄ downloads ‚îú‚îÄ‚îÄ main.py ‚îú‚îÄ‚îÄ requirements.txt ‚îî‚îÄ‚îÄ venv ‚îú‚îÄ‚îÄ ... ‚îî‚îÄ‚îÄ ... 2.1. Source folder This folder contains all the application related logic. This is intentional, as I like separating the actual application code from the repository related code. This has the extra benefit of easing the reusing/copy-pasting of the source code into other projects.\n2.2. Scheduling folder This folder contains all the scheduling related scripts (shell scripts, dags, \u0026hellip;). I know this is not a common practice, as I have seen many teams keep their scheduling scripts in a separate repository (if any). Personally, I am not a big fan of this approach as if a migration or refactoring is about to happen (which happens more often that we like to admit), the programmer would need to be aware of how, from where and in which order the application is being run. This easily becomes a bottleneck as the programmer would need to start requesting access to the scheduling tools or start setting meetings with the scheduling team to clarify how everything works before s/he can even begin working!!.\n2.3. Downloads folder This folder is a placeholder for where all the scraped/downloaded files will go. (note that it may contain subfolders depending on the project). However, by keeping it in a separate folder, I can easily replace this location with another one when running the applicatoin as a container (via volumes property of docker). This way I can easily test my projects locally and easily deploy them in docker with minimal code change needed.\n","permalink":"https://daguirreag.github.io/posts/webscraping-project-structure/","summary":"As the amount of personal webscraping projects increased, I realized the importance of having an structured project template. The reason being that it allows to systematically create new projects without having to worry about refactoring later on if it doesn\u0026rsquo;t match with my webscraping service.\nBecause of this, I have decided to explain in this post how I structure most of my webscraping projects:\n1. Considerations I like to keep each project as independent from the rest as possible.","title":"Webscraping project structure"},{"content":"As a data engineer is quite common to be on the lookup for useful (or at least interesting) websites to scrape data from.\nHowever, it is quite common to have different projects to start the same (e.g. downloading the HTML from the internet) but to end up processing them (ETL) in different ways.\nBecause of this, I have setup a simple Raspberry Pi as my centralized webscraping service where:\nWebsites, files, \u0026hellip; can be downloaded into a raw format. Raw files processing can be done in the right order. Data is made available for analytics/reporting/\u0026hellip; Monitoring capabilities are available. 1. Requirements For this to work, I identified the following tools as basic requirements:\nAiflow (Orchestration tool): As each project\u0026rsquo;s logic may be different (how and in which order files need to be processed), an orchestration tool is needed to keep track of the status of each project as well as ensure the right order of execution. Selenium (Browser automation tool): As most of the projects will require of downloading data from the web, a browser automation tool is needed to avoid spawning the same browser automation service. Docker compose: To keep everything consistent, isolated and easily replicable, I will be setting everything via a single Docker compose file. This allows me to easily manage all services, as well as ensure the services are discoverable between them (as docker attaches ALL the containers in the SAME DOCKER COMPOSE FILE to the SAME NETWORK). 2. Webscraping service architecture As the webscraping service will be installed in a single host (for the time being at least), the architecture will remain very simple. Find below an overview of the architecture:\nOverview of the webscraping service architecture. (Note that my idea is to modify the architecture to a micro-service oriented architecture once the amount of projects starts becoming bigger. However, for the time being the proposed architecture should be enough for my needs, as a micro-service architecture raises unique challenges (registering the services, making the services discoverable, managing different docker instances, \u0026hellip;).)\n2.1 Design choices I will like to keep Airflow as independent as possible, just adding copying the dags to its folder from the original repository. I decided to go for this approach, because as the number of projects increases, it will become very hard to know where a particular dag belongs to, and as each project contains its own order/logic, it is smart to keep as close as possible to the source code.\n3. Setting the service As mentioned in the introduction section, as all the services will be living in the same host machine, one docker compose file should be enough to setup everything correctly. However, certain steps must be followed before starting up the docker containers.\n3.1. Preparation 3.1.1. Preparing Airflow I will start by cding to the location where Airflow installation will reside. Once there, I will start by creating the following file:\ndocker-compose.yml # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \u0026#34;License\u0026#34;); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # # Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL. # # WARNING: This configuration is for local development. Do not use it in a production deployment. # # This configuration supports basic configuration using environment variables or an .env file # The following variables are supported: # # AIRFLOW_IMAGE_NAME - Docker image name used to run Airflow. # Default: apache/airflow:2.7.3 # AIRFLOW_UID - User ID in Airflow containers # Default: 50000 # AIRFLOW_PROJ_DIR - Base path to which all the files will be volumed. # Default: . # Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode # # _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account (if requested). # Default: airflow # _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account (if requested). # Default: airflow # _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers. # Use this option ONLY for quick checks. Installing requirements at container # startup is done EVERY TIME the service is started. # A better way is to build a custom image or extend the official image # as described in https://airflow.apache.org/docs/docker-stack/build.html. # Default: \u0026#39;\u0026#39; # # Feel free to modify this file to suit your needs. --- version: \u0026#39;3.8\u0026#39; x-airflow-common: \u0026amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \u0026#34;build\u0026#34; line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3} # build: . environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # For backward compatibility, with Airflow \u0026lt;2.3 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0 AIRFLOW__CORE__FERNET_KEY: \u0026#39;\u0026#39; AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#39;true\u0026#39; AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#39;false\u0026#39; AIRFLOW__API__AUTH_BACKENDS: \u0026#39;airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\u0026#39; # yamllint disable rule:line-length # Use simple http server on scheduler for health checks # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server # yamllint enable rule:line-length AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \u0026#39;true\u0026#39; # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks # for other purpose (development, test and especially production usage) build/extend Airflow image. _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} volumes: - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; depends_on: \u0026amp;airflow-common-depends-on redis: condition: service_healthy postgres: condition: service_healthy services: #### Non-Airflow related containers #### #### Airflow related containers #### postgres: image: postgres:13 environment: POSTGRES_USER: airflow POSTGRES_PASSWORD: airflow POSTGRES_DB: airflow volumes: - postgres-db-volume:/var/lib/postgresql/data healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] interval: 10s retries: 5 start_period: 5s restart: always ports: - \u0026#34;5432:5432\u0026#34; expose: - \u0026#34;5432\u0026#34; redis: image: redis:latest expose: - 6379 healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;redis-cli\u0026#34;, \u0026#34;ping\u0026#34;] interval: 10s timeout: 30s retries: 50 start_period: 30s restart: always airflow-webserver: \u0026lt;\u0026lt;: *airflow-common command: webserver ports: - \u0026#34;8080:8080\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-scheduler: \u0026lt;\u0026lt;: *airflow-common command: scheduler healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:8974/health\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-worker: \u0026lt;\u0026lt;: *airflow-common command: celery worker healthcheck: # yamllint disable rule:line-length test: - \u0026#34;CMD-SHELL\u0026#34; - \u0026#39;celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \u0026#34;celery@$${HOSTNAME}\u0026#34; || celery --app airflow.executors.celery_executor.app inspect ping -d \u0026#34;celery@$${HOSTNAME}\u0026#34;\u0026#39; interval: 30s timeout: 10s retries: 5 start_period: 30s environment: \u0026lt;\u0026lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: \u0026#34;0\u0026#34; restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-triggerer: \u0026lt;\u0026lt;: *airflow-common command: triggerer healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#39;airflow jobs check --job-type TriggererJob --hostname \u0026#34;$${HOSTNAME}\u0026#34;\u0026#39;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-init: \u0026lt;\u0026lt;: *airflow-common entrypoint: /bin/bash # yamllint disable rule:line-length command: - -c - | function ver() { printf \u0026#34;%04d%04d%04d%04d\u0026#34; $${1//./ } } airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO \u0026amp;\u0026amp; gosu airflow airflow version) airflow_version_comparable=$$(ver $${airflow_version}) min_airflow_version=2.2.0 min_airflow_version_comparable=$$(ver $${min_airflow_version}) if (( airflow_version_comparable \u0026lt; min_airflow_version_comparable )); then echo echo -e \u0026#34;\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\e[0m\u0026#34; echo \u0026#34;The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!\u0026#34; echo exit 1 fi if [[ -z \u0026#34;${AIRFLOW_UID}\u0026#34; ]]; then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\u0026#34; echo \u0026#34;If you are on Linux, you SHOULD follow the instructions below to set \u0026#34; echo \u0026#34;AIRFLOW_UID environment variable, otherwise files will be owned by root.\u0026#34; echo \u0026#34;For other operating systems you can get rid of the warning with manually created .env file:\u0026#34; echo \u0026#34; See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\u0026#34; echo fi one_meg=1048576 mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg)) cpus_available=$$(grep -cE \u0026#39;cpu[0-9]+\u0026#39; /proc/stat) disk_available=$$(df / | tail -1 | awk \u0026#39;{print $$4}\u0026#39;) warning_resources=\u0026#34;false\u0026#34; if (( mem_available \u0026lt; 4000 )) ; then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\u0026#34; echo \u0026#34;At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\u0026#34; echo warning_resources=\u0026#34;true\u0026#34; fi if (( cpus_available \u0026lt; 2 )); then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\u0026#34; echo \u0026#34;At least 2 CPUs recommended. You have $${cpus_available}\u0026#34; echo warning_resources=\u0026#34;true\u0026#34; fi if (( disk_available \u0026lt; one_meg * 10 )); then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\u0026#34; echo \u0026#34;At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\u0026#34; echo warning_resources=\u0026#34;true\u0026#34; fi if [[ $${warning_resources} == \u0026#34;true\u0026#34; ]]; then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\u0026#34; echo \u0026#34;Please follow the instructions to increase amount of resources available:\u0026#34; echo \u0026#34; https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\u0026#34; echo fi mkdir -p /sources/logs /sources/dags /sources/plugins chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins} exec /entrypoint airflow version # yamllint enable rule:line-length environment: \u0026lt;\u0026lt;: *airflow-common-env _AIRFLOW_DB_MIGRATE: \u0026#39;true\u0026#39; _AIRFLOW_WWW_USER_CREATE: \u0026#39;true\u0026#39; _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} _PIP_ADDITIONAL_REQUIREMENTS: \u0026#39;\u0026#39; user: \u0026#34;0:0\u0026#34; volumes: - ${AIRFLOW_PROJ_DIR:-.}:/sources airflow-cli: \u0026lt;\u0026lt;: *airflow-common profiles: - debug environment: \u0026lt;\u0026lt;: *airflow-common-env CONNECTION_CHECK_MAX_COUNT: \u0026#34;0\u0026#34; # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252 command: - bash - -c - airflow # You can enable flower by adding \u0026#34;--profile flower\u0026#34; option e.g. docker-compose --profile flower up # or by explicitly targeted on the command line e.g. docker-compose up flower. # See: https://docs.docker.com/compose/profiles/ flower: \u0026lt;\u0026lt;: *airflow-common command: celery flower profiles: - flower ports: - \u0026#34;5555:5555\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:5555/\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully volumes: postgres-db-volume: Afterwards, I will create the required Airflow folders and files in the same location:\nmkdir -p ./dags ./logs ./plugins ./config echo -e \u0026#34;AIRFLOW_UID=$(id -u)\u0026#34; \u0026gt; .env After previous steps, the folder structure will look like this:\n‚îî‚îÄ‚îÄ Airflow ‚îú‚îÄ‚îÄ config ‚îú‚îÄ‚îÄ dags ‚îú‚îÄ‚îÄ docker-compose.yaml ‚îú‚îÄ‚îÄ .env ‚îú‚îÄ‚îÄ logs ‚îî‚îÄ‚îÄ plugins Once the folder structure is created, we can proceed to initialize Airflow:\ndocker compose up airflow-init (More detailed information in here)\n3.1.2. Preparing Project I will start by cding to the location where projects\u0026rsquo; repos will reside. Once there I will start by cloning the repositories. In my case, the folder structure will look like this:\n‚îî‚îÄ‚îÄ GitHub ‚îî‚îÄ‚îÄ project_1 ‚îú‚îÄ‚îÄ dockerfile ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ requirements.txt ‚îî‚îÄ‚îÄ main.py Example files dockerfile\nFROM python:3.10-bullseye WORKDIR /app COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY . ./ CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] docker-compose.yml\nservices: project_1: image: project_1 container_name: project_1 volumes: - ./:/app/downloads ports: - \u0026#34;8101:80\u0026#34; expose: - \u0026#34;80\u0026#34; restart: always main.py\nfrom fastapi import FastAPI from starlette.responses import RedirectResponse import datetime import os app = FastAPI() @app.get(\u0026#34;/\u0026#34;) def main(): response = RedirectResponse(url=\u0026#39;/docs\u0026#39;) return response @app.get(\u0026#34;/create_file\u0026#34;) def create_file(): # Get filename filename_prefix = \u0026#39;file\u0026#39; filename_suffix = str(datetime.datetime.now()).replace(\u0026#39;:\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;-\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39; \u0026#39;,\u0026#39;_\u0026#39;).split(\u0026#39;.\u0026#39;)[0] filename = \u0026#39;_\u0026#39;.join([filename_prefix, filename_suffix]) + \u0026#39;.txt\u0026#39; filepath = \u0026#39;downloads\u0026#39; # Create file f = open(os.path.join(filepath, filename), \u0026#34;a\u0026#34;) f.write(\u0026#34;Hello world!!\u0026#34;) f.close() return None requirement.txt\nannotated-types==0.6.0 anyio==3.7.1 click==8.1.7 exceptiongroup==1.2.0 fastapi==0.104.1 h11==0.14.0 idna==3.6 pydantic==2.5.2 pydantic-core==2.14.5 sniffio==1.3.0 starlette==0.27.0 typing-extensions==4.8.0 uvicorn==0.24.0.post1 With the repository cloned I will build the image of the project by typing:\ndocker build -t project_1 . (Note that I have decided to manually build the images every time something changes as it is the easiest way to manage it this way. Ideally I would like to have an automatic CI/CD workflow setup that will check and deploy the changes once done, but due to the nature of my projects (which most are only developed by me), it wouldn\u0026rsquo;t add a lot of value.)\n3.1.3. Adding a project and selenium to the Docker-compose With Airflow and the project image ready, it is time to add the the project to the docker-compose.yml file, so it can be started up/shut down at the same time as the other services.\nStart by opening the location of the docker-compose.yml. In this file, I will locate the following lines:\nservices: #### Non-Airflow related containers #### #### Airflow related containers #### postgres: image: postgres:13 environment: POSTGRES_USER: airflow POSTGRES_PASSWORD: airflow ... Once the section where the extra services will be defined is located, I will proceed to add the following lines:\nselenium_standalone_chrome: image: selenium/standalone-chrome #image: seleniarm/standalone-chromium:latest # Use this when running in an ARM architecture machine (like a Raspberry Pi) privileged: true shm_size: 2g ports: - \u0026#34;4444:4444\u0026#34; expose: - \u0026#34;4444\u0026#34; project_1: image: project_1 container_name: project_1 volumes: - /home/daniel/Documents/Datalake/project_1:/app/downloads ports: - \u0026#34;8101:80\u0026#34; expose: - \u0026#34;80\u0026#34; restart: always The docker-compose.yml file will look like this: docker-compose.yml # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \u0026#34;License\u0026#34;); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # # Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL. # # WARNING: This configuration is for local development. Do not use it in a production deployment. # # This configuration supports basic configuration using environment variables or an .env file # The following variables are supported: # # AIRFLOW_IMAGE_NAME - Docker image name used to run Airflow. # Default: apache/airflow:2.7.3 # AIRFLOW_UID - User ID in Airflow containers # Default: 50000 # AIRFLOW_PROJ_DIR - Base path to which all the files will be volumed. # Default: . # Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode # # _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account (if requested). # Default: airflow # _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account (if requested). # Default: airflow # _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers. # Use this option ONLY for quick checks. Installing requirements at container # startup is done EVERY TIME the service is started. # A better way is to build a custom image or extend the official image # as described in https://airflow.apache.org/docs/docker-stack/build.html. # Default: \u0026#39;\u0026#39; # # Feel free to modify this file to suit your needs. --- version: \u0026#39;3.8\u0026#39; x-airflow-common: \u0026amp;airflow-common # In order to add custom dependencies or upgrade provider packages you can use your extended image. # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml # and uncomment the \u0026#34;build\u0026#34; line below, Then run `docker-compose build` to build the images. image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3} # build: . environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow # For backward compatibility, with Airflow \u0026lt;2.3 AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0 AIRFLOW__CORE__FERNET_KEY: \u0026#39;\u0026#39; AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \u0026#39;true\u0026#39; AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#39;false\u0026#39; AIRFLOW__API__AUTH_BACKENDS: \u0026#39;airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\u0026#39; # yamllint disable rule:line-length # Use simple http server on scheduler for health checks # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server # yamllint enable rule:line-length AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \u0026#39;true\u0026#39; # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks # for other purpose (development, test and especially production usage) build/extend Airflow image. _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-} volumes: - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins user: \u0026#34;${AIRFLOW_UID:-50000}:0\u0026#34; depends_on: \u0026amp;airflow-common-depends-on redis: condition: service_healthy postgres: condition: service_healthy services: #### Non-Airflow related containers #### selenium_standalone_chrome: image: selenium/standalone-chrome #image: seleniarm/standalone-chromium:latest # Use this when running in an ARM architecture machine (like a Raspberry Pi) privileged: true shm_size: 2g ports: - \u0026#34;4444:4444\u0026#34; expose: - \u0026#34;4444\u0026#34; project_1: image: project_1 container_name: project_1 volumes: - /home/daniel/Documents/Datalake/project_1:/app/downloads ports: - \u0026#34;8101:80\u0026#34; expose: - \u0026#34;80\u0026#34; restart: always #### Airflow related containers #### postgres: image: postgres:13 environment: POSTGRES_USER: airflow POSTGRES_PASSWORD: airflow POSTGRES_DB: airflow volumes: - postgres-db-volume:/var/lib/postgresql/data healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;pg_isready\u0026#34;, \u0026#34;-U\u0026#34;, \u0026#34;airflow\u0026#34;] interval: 10s retries: 5 start_period: 5s restart: always ports: - \u0026#34;5432:5432\u0026#34; expose: - \u0026#34;5432\u0026#34; redis: image: redis:latest expose: - 6379 healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;redis-cli\u0026#34;, \u0026#34;ping\u0026#34;] interval: 10s timeout: 30s retries: 50 start_period: 30s restart: always airflow-webserver: \u0026lt;\u0026lt;: *airflow-common command: webserver ports: - \u0026#34;8080:8080\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-scheduler: \u0026lt;\u0026lt;: *airflow-common command: scheduler healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:8974/health\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-worker: \u0026lt;\u0026lt;: *airflow-common command: celery worker healthcheck: # yamllint disable rule:line-length test: - \u0026#34;CMD-SHELL\u0026#34; - \u0026#39;celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \u0026#34;celery@$${HOSTNAME}\u0026#34; || celery --app airflow.executors.celery_executor.app inspect ping -d \u0026#34;celery@$${HOSTNAME}\u0026#34;\u0026#39; interval: 30s timeout: 10s retries: 5 start_period: 30s environment: \u0026lt;\u0026lt;: *airflow-common-env # Required to handle warm shutdown of the celery workers properly # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation DUMB_INIT_SETSID: \u0026#34;0\u0026#34; restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-triggerer: \u0026lt;\u0026lt;: *airflow-common command: triggerer healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#39;airflow jobs check --job-type TriggererJob --hostname \u0026#34;$${HOSTNAME}\u0026#34;\u0026#39;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully airflow-init: \u0026lt;\u0026lt;: *airflow-common entrypoint: /bin/bash # yamllint disable rule:line-length command: - -c - | function ver() { printf \u0026#34;%04d%04d%04d%04d\u0026#34; $${1//./ } } airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO \u0026amp;\u0026amp; gosu airflow airflow version) airflow_version_comparable=$$(ver $${airflow_version}) min_airflow_version=2.2.0 min_airflow_version_comparable=$$(ver $${min_airflow_version}) if (( airflow_version_comparable \u0026lt; min_airflow_version_comparable )); then echo echo -e \u0026#34;\\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\\e[0m\u0026#34; echo \u0026#34;The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!\u0026#34; echo exit 1 fi if [[ -z \u0026#34;${AIRFLOW_UID}\u0026#34; ]]; then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m\u0026#34; echo \u0026#34;If you are on Linux, you SHOULD follow the instructions below to set \u0026#34; echo \u0026#34;AIRFLOW_UID environment variable, otherwise files will be owned by root.\u0026#34; echo \u0026#34;For other operating systems you can get rid of the warning with manually created .env file:\u0026#34; echo \u0026#34; See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\u0026#34; echo fi one_meg=1048576 mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg)) cpus_available=$$(grep -cE \u0026#39;cpu[0-9]+\u0026#39; /proc/stat) disk_available=$$(df / | tail -1 | awk \u0026#39;{print $$4}\u0026#39;) warning_resources=\u0026#34;false\u0026#34; if (( mem_available \u0026lt; 4000 )) ; then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m\u0026#34; echo \u0026#34;At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))\u0026#34; echo warning_resources=\u0026#34;true\u0026#34; fi if (( cpus_available \u0026lt; 2 )); then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m\u0026#34; echo \u0026#34;At least 2 CPUs recommended. You have $${cpus_available}\u0026#34; echo warning_resources=\u0026#34;true\u0026#34; fi if (( disk_available \u0026lt; one_meg * 10 )); then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m\u0026#34; echo \u0026#34;At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))\u0026#34; echo warning_resources=\u0026#34;true\u0026#34; fi if [[ $${warning_resources} == \u0026#34;true\u0026#34; ]]; then echo echo -e \u0026#34;\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m\u0026#34; echo \u0026#34;Please follow the instructions to increase amount of resources available:\u0026#34; echo \u0026#34; https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\u0026#34; echo fi mkdir -p /sources/logs /sources/dags /sources/plugins chown -R \u0026#34;${AIRFLOW_UID}:0\u0026#34; /sources/{logs,dags,plugins} exec /entrypoint airflow version # yamllint enable rule:line-length environment: \u0026lt;\u0026lt;: *airflow-common-env _AIRFLOW_DB_MIGRATE: \u0026#39;true\u0026#39; _AIRFLOW_WWW_USER_CREATE: \u0026#39;true\u0026#39; _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow} _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} _PIP_ADDITIONAL_REQUIREMENTS: \u0026#39;\u0026#39; user: \u0026#34;0:0\u0026#34; volumes: - ${AIRFLOW_PROJ_DIR:-.}:/sources airflow-cli: \u0026lt;\u0026lt;: *airflow-common profiles: - debug environment: \u0026lt;\u0026lt;: *airflow-common-env CONNECTION_CHECK_MAX_COUNT: \u0026#34;0\u0026#34; # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252 command: - bash - -c - airflow # You can enable flower by adding \u0026#34;--profile flower\u0026#34; option e.g. docker-compose --profile flower up # or by explicitly targeted on the command line e.g. docker-compose up flower. # See: https://docs.docker.com/compose/profiles/ flower: \u0026lt;\u0026lt;: *airflow-common command: celery flower profiles: - flower ports: - \u0026#34;5555:5555\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:5555/\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully volumes: postgres-db-volume: Finally, I test that everything is working by typing:\ndocker compose up 3.1.4. Adding customs dags to Airflow With the services running, it is time to add the a DAG to Airflow. In order to do so, I just need to add the following dag file to the dag folder.\nproject_1_dag.py\nfrom airflow import DAG from airflow.operators.python import PythonOperator, BranchPythonOperator from airflow.operators.bash import BashOperator from datetime import datetime import requests PORT = 80 def call_api(url: str): response = requests.get(url) if not (200\u0026lt;=response.status_code\u0026lt;=299): raise Exception(\u0026#39;Something went wrong.\u0026#39;) def _create_file(): url = f\u0026#34;http://project_1:{PORT}/create_file\u0026#34; call_api(url) with DAG(\u0026#34;project_1_dag\u0026#34;, # Dag id start_date=datetime(2023, 10 ,5), # start date, the 1st of January 2023 schedule=\u0026#39;@weekly\u0026#39;, # Cron expression, here @daily means once every day. catchup=False): # Tasks are implemented under the dag object create_file = PythonOperator( task_id=\u0026#34;create_file\u0026#34;, python_callable=_create_file ) create_file Notice how as all the services belong to the same network, I can call each service by typing the service\u0026rsquo;s name directly: \u0026quot;http://project_1:{PORT}/create_file\u0026quot;.\nSuccessful Airflow run. 3.2. Running the services With everything ready, starting and shutting down everything becomes extremely simple. It only requires of typing the following in the location of my docker-compose.yml file:\ndocker compose up Just for explanation\u0026rsquo;s shake, this is how my current setup looks like:\n‚îú‚îÄ‚îÄ Airflow ‚îÇ¬†‚îú‚îÄ‚îÄ config ‚îÇ¬†‚îú‚îÄ‚îÄ dags ‚îÇ¬†‚îú‚îÄ‚îÄ docker-compose.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ .env ‚îÇ¬†‚îú‚îÄ‚îÄ logs ‚îÇ¬†‚îî‚îÄ‚îÄ plugins ‚îî‚îÄ‚îÄ GitHub ‚îÇ¬†‚îú‚îÄ‚îÄ project_1 ‚îÇ¬†‚îú‚îÄ‚îÄ project_2 ‚îÇ¬†‚îî‚îÄ‚îÄ project_3 ‚îî‚îÄ‚îÄ Datalake ‚îú‚îÄ‚îÄ project_1 ‚îú‚îÄ‚îÄ project_2 ‚îî‚îÄ‚îÄ project_3 4. Troubleshooting Although most services should be accessible via a browser (the docker ports will be forwarded to host\u0026rsquo;s ports), there may be situations in which they won\u0026rsquo;t work. For these instances, it is a good idea to have a troubleshooting docker image ready that could connect to the troubling containers. I like to use the nicolaka/netshoot image to troubleshoot as it contains most of the tools you will need when troubleshooting network related issues.\nTo troubleshoot an already running docker container, attach the netshoot container to that container\u0026rsquo;s network by typing:\ndocker run -it --net container:\u0026lt;container_name\u0026gt; nicolaka/netshoot Notes on Airflow For simplicity shake, the airflow logs and dags are located in the same directory as the compose file. This may become an issue if a heavy use of Airflow is done (as it logs will start taking too much space). Having dags into the same location is also not ideal, as there is no easy way on putting those dags under version control. However, for my use cases, these both issues are very unlikely, so I decided to go for the easier setup.\nAbove points will be considered (if/when) my new micro-service centered architecture is setup. As the main idea behind that architecture would be to increase the scalability and fault-tolerant of the system.\n","permalink":"https://daguirreag.github.io/posts/setting-up-a-webscraping-service/","summary":"As a data engineer is quite common to be on the lookup for useful (or at least interesting) websites to scrape data from.\nHowever, it is quite common to have different projects to start the same (e.g. downloading the HTML from the internet) but to end up processing them (ETL) in different ways.\nBecause of this, I have setup a simple Raspberry Pi as my centralized webscraping service where:\nWebsites, files, \u0026hellip; can be downloaded into a raw format.","title":"Setting up a webscraping service"},{"content":"With the homelab hardware set up, it is time to start preparing the machines so they can work together. As I intend to use my homelab as a safe and easy to use \u0026ldquo;IT playground\u0026rdquo;, I have decided that the following elements would need to be setup:\nStatic IP address on each machine. This facilitates troubleshooting as well as easing the communication between the machines. Remote control software and SSH. This facilitates troubleshooting my \u0026ldquo;remote\u0026rdquo; machines without having to hook a keyboard+mouse+monitor to them. An Ads and Internet tracker blocking service. This should add extra network protection. Containerization software. This should facilitate the deployment of code as well as reduce issues with package dependencies. 1. Setting up static IP address (in Raspbian) Setting up a static address in a Raspbian machine is quite straightforward.\nGet Raspberry Pi\u0026rsquo;s IP address by typing: hostname -I. Get my router\u0026rsquo;s IP address by typing: ip r Get Raspberry Pi\u0026rsquo;s DNS IP address by typing: nano /etc/resolv.conf Edit the following lines in the dhcpcd.conf file by typing: nano /etc/dhcpcd.conf: interface eth0 static ip_address=192.168.0.132/24 static routers=192.168.0.1 static domain_name_servers=192.168.0.1 (Note that your IP address values may be different) Save changes by pressing ctrl + x. Restart the machine. (Optional) Add the static IP address tags to the machines. Static IP address tags. 2. Setting SSH and Remote control software Follow next steps:\nClick main menu. Click Preferences. Click Raspberry Pi Configuration. Select the tab Interfaces. Activate SSH and VNC. (Note that in order to connect via VNC, I will have to install a VNC remote control software. In my case, I use VNC viewer due to its simplicity and availability in my Ubuntu OS).\nEnabling SSH and VNC. 3. Installing Ads and Internet tracker blocking service (Pi-Hole) Follow next steps:\nVisit https://docs.pi-hole.net/main/basic-install/ and follow the instructions. Visit https://firebog.net/ and note down the lists (urls) wanted to be added into Pi-Hole. Visit Pi-Hole\u0026rsquo;s address (http://192.168.0.132/admin in my case). Add noted down urls in the Adlist section. Open a terminal window to add recently added lists to Pi-Hole by typing: pihole -g. Go into your specific router\u0026rsquo;s admin page and make sure to setup the Pi-Hole\u0026rsquo;s IP address as your primary DNS. 4. Installing Containerization software (Docker) Follow next steps:\nUpdate package manager index: sudo apt update Upgrade packages: sudo apt upgrade Download Docker \u0026ldquo;installer\u0026rdquo; script: curl -fsSL https://get.docker.com -o get-docker.sh Execute Docker \u0026ldquo;installer\u0026rdquo; script: sudo ./get-docker.sh (Optional) Add user to Docker usergorub to avoid having to type sudo everytime a command needs to be executed: sudo usermod -aG docker [username] ","permalink":"https://daguirreag.github.io/posts/setting-up-my-own-home-lab-software/","summary":"With the homelab hardware set up, it is time to start preparing the machines so they can work together. As I intend to use my homelab as a safe and easy to use \u0026ldquo;IT playground\u0026rdquo;, I have decided that the following elements would need to be setup:\nStatic IP address on each machine. This facilitates troubleshooting as well as easing the communication between the machines. Remote control software and SSH. This facilitates troubleshooting my \u0026ldquo;remote\u0026rdquo; machines without having to hook a keyboard+mouse+monitor to them.","title":"Setting up my own Home lab: Software"},{"content":"As a data engineer, it is quite obvious that I like tinkering with technology, specially with software related stuff. Because of this, some time ago I decided to setup my own simple homelab where I could experiment and run scripts around without worrying about setting up cloud services.\nAs such, I have decided to share how I set up my homelab, so others can use it as an inspiration or just have a look at it.\n1. Requirements No project can start without the prior requirement gathering. Next I am displaying the main points I want to get in my homelab:\nHardware-wise Low power consumption (as I plan to have projects running 24/7). Low noise (as I will place it near my living room). Low cost setup. Software-wise Monitor network activity. Run projects 24/7. 2. Hardware Based on the requirements and a little bit of research (and availability of the devices), I settled down for the following:\nRaspberry Pi 4 Model B (8Gb ram) Raspberry Pi 3 Model B + TP Link TL-WR940N (Router) Main Desktop PC Seagate Expansion Portable Hard Drive SRD0NF1 (external HDD) Homelab hardware. 3. Network architecture/topology The devices communicate with each other via ethernet connection, all of them connected through a centralized router. Note that even though the router is a single Point-Of-Failure, it is very unlikely to happen, and none of the services I am running are critical enough to consider for a redundant solution. Find below the topology of my homelab:\nOverview of Network topology. Future steps Ideally, I would like to have another Raspberry Pi 4 fully dedicated to experimentation. This should allow me to identify issues that may arise when deploying services in my 24/7 running Raspberry Pi 4, without risking breaking it.\n","permalink":"https://daguirreag.github.io/posts/setting-up-my-own-home-lab-hardware-networking/","summary":"As a data engineer, it is quite obvious that I like tinkering with technology, specially with software related stuff. Because of this, some time ago I decided to setup my own simple homelab where I could experiment and run scripts around without worrying about setting up cloud services.\nAs such, I have decided to share how I set up my homelab, so others can use it as an inspiration or just have a look at it.","title":"Setting up my own Home lab: Hardware/Networking"},{"content":"Hi and welcome to my personal blog, my name is Daniel!\nI am a Data Engineer who lives and works in the Netherlands üá≥üá±. However, previously I worked as a production engineer in China for 4 years, before spending some time as a robotics engineer in Spain.\nMy expertise lies in the realms of data engineering, software engineering, and project management, where I drive entd-to-end projects aligned with business needs.\nIf you want to connect with me or just discuss anything, just drop me a message on any of my other Social Media platforms (links below).\nContact me LinkedIn GitHub ","permalink":"https://daguirreag.github.io/about/","summary":"Hi and welcome to my personal blog, my name is Daniel!\nI am a Data Engineer who lives and works in the Netherlands üá≥üá±. However, previously I worked as a production engineer in China for 4 years, before spending some time as a robotics engineer in Spain.\nMy expertise lies in the realms of data engineering, software engineering, and project management, where I drive entd-to-end projects aligned with business needs.","title":"About"},{"content":"Data engineering üíΩ Financial news collectors - Financial news collector and aggregator service. IP-to-Geolocation - Proof of concept of a IP-to-Geolocation web application. It returns the geographic location of a given IP address. Software engineering üßë‚Äçüíª STL to gif - A Python script to create a gif file from a STL file. URL-shortener - Proof of concept of a URL shortener application. Feedback-functionality - A simple implementation to add the functionality to collect user provided feedback via website forms to your websites. Hardware üîß CNC pen-plotter - A repository of the code, schematics and hardware needed to build a simple two linear axes plotter controlled with an Arduino and a CNC shield. 3D-printable bearing generator - A python script to generate 3D printable bearings according to your dimensional constrains. GCode-to-ABB - A python implementation for transforming a GCode file to a ABB robot specific movements. ","permalink":"https://daguirreag.github.io/projects/","summary":"Data engineering üíΩ Financial news collectors - Financial news collector and aggregator service. IP-to-Geolocation - Proof of concept of a IP-to-Geolocation web application. It returns the geographic location of a given IP address. Software engineering üßë‚Äçüíª STL to gif - A Python script to create a gif file from a STL file. URL-shortener - Proof of concept of a URL shortener application. Feedback-functionality - A simple implementation to add the functionality to collect user provided feedback via website forms to your websites.","title":"Projects"}]