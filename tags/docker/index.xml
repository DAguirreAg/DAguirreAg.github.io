<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Docker on Daniel Aguirre</title>
    <link>https://daguirreag.github.io/tags/docker/</link>
    <description>Recent content in Docker on Daniel Aguirre</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Dec 2023 17:53:27 +0100</lastBuildDate><atom:link href="https://daguirreag.github.io/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Webscraping project structure</title>
      <link>https://daguirreag.github.io/posts/webscraping-project-structure/</link>
      <pubDate>Mon, 04 Dec 2023 17:53:27 +0100</pubDate>
      
      <guid>https://daguirreag.github.io/posts/webscraping-project-structure/</guid>
      <description>As the amount of personal webscraping projects increased, I realized the importance of having an structured project template. The reason being that it allows to systematically create new projects without having to worry about refactoring later on if it doesn&amp;rsquo;t match with my webscraping service.
Because of this, I have decided to explain in this post how I structure most of my webscraping projects:
1. Considerations I like to keep each project as independent from the rest as possible.</description>
    </item>
    
    <item>
      <title>Setting up a webscraping service</title>
      <link>https://daguirreag.github.io/posts/setting-up-a-webscraping-service/</link>
      <pubDate>Sat, 02 Dec 2023 22:12:31 +0100</pubDate>
      
      <guid>https://daguirreag.github.io/posts/setting-up-a-webscraping-service/</guid>
      <description>As a data engineer is quite common to be on the lookup for useful (or at least interesting) websites to scrape data from.
However, it is quite common to have different projects to start the same (e.g. downloading the HTML from the internet) but to end up processing them (ETL) in different ways.
Because of this, I have setup a simple Raspberry Pi as my centralized webscraping service where:
Websites, files, &amp;hellip; can be downloaded into a raw format.</description>
    </item>
    
    <item>
      <title>Setting up my own Home lab: Software</title>
      <link>https://daguirreag.github.io/posts/setting-up-my-own-home-lab-software/</link>
      <pubDate>Sat, 25 Nov 2023 18:51:39 +0100</pubDate>
      
      <guid>https://daguirreag.github.io/posts/setting-up-my-own-home-lab-software/</guid>
      <description>With the homelab hardware set up, it is time to start preparing the machines so they can work together. As I intend to use my homelab as a safe and easy to use &amp;ldquo;IT playground&amp;rdquo;, I have decided that the following elements would need to be setup:
Static IP address on each machine. This facilitates troubleshooting as well as easing the communication between the machines. Remote control software and SSH. This facilitates troubleshooting my &amp;ldquo;remote&amp;rdquo; machines without having to hook a keyboard+mouse+monitor to them.</description>
    </item>
    
  </channel>
</rss>
